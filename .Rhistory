plot_between_cluster_distance <- function(centroids, title) {
x <- get_between_cluster_dist(centroids)
y <- as.data.frame(x)
colnames(y) <- "Distance"
Cluster <- rownames(y)
line <- ggplot(y, aes(x=Cluster, y=Distance, group=1)) +
geom_line() + geom_path() + geom_point() +
ggtitle(paste(c("Between Cluster Distance -", title), collapse=" ")) +
ylim(1, 5)
return(line)
}
do_elbow_plot(lux.num.ALL)
# Assign the "optimal" number to a variable
C<- 8
abline(v = C, lty =2)
all.kmeans <- do_k_means_proper(lux.num.ALL, C)
all.scoreperc <- do_k_means(lux.num.ALL, C, all.kmeans)
packages = c('readr', 'fmsb', 'data.table', 'reshape2', 'cluster', 'dplyr', 'qdapTools', 'mltools', 'ggplot2')
for (p in packages){
if(!require(p, character.only = T)){
install.packages(p)
}
library(p,character.only = T)
}
# Set seed
set.seed(123)
lux_raw <- read_csv(file="data/luxehorecaFSA.csv",
col_types = cols(CLM_DT    = col_date(format = "%d/%m/%y"),
RCPT_DT   = col_date(format = "%d/%m/%y"),
REIMB_DT  = col_date(format = "%d/%m/%y"),
EA_DT     = col_date(format = "%d/%m/%y")))
lux_validated_raw <- read_csv(file="data/luxehorecaFSA(validated).csv",
col_types = cols(CLM_DT    = col_date(format = "%d/%m/%y"),
RCPT_DT   = col_date(format = "%d/%m/%y"),
REIMB_DT  = col_date(format = "%d/%m/%y"),
EA_DT     = col_date(format = "%d/%m/%y")))
# Remove the missing row
lux_raw <- lux_raw[!(lux_raw$FSA_TYP != 'FLXI' & !is.na(lux_raw$FLEXBEN_TYPE)),]
#Merge the 2 claim columns
lux_raw$FLEXBEN_TYPE <- lux_raw$FLEXBEN_TYPE %>% replace_na("") # Replace all na with empty string
lux_raw$CLAIM_TYPES <- paste(lux_raw$FSA_TYP, lux_raw$FLEXBEN_TYPE) # Adds a new column called claims type
#creates 3 raw sets for binding later
lux_raw_flxi <- filter(lux_raw, lux_raw$FSA_TYP == "FLXI")
lux_raw_birt <- filter(lux_raw, lux_raw$FSA_TYP == "BIRT")
lux_raw_miis <- filter(lux_raw, lux_raw$FSA_TYP == "MIIS")
data_preprocessing <- function(lux_raw_data_input) {
to_remove <- c('EMP_ID', 'CLM_REF', 'CLM_SYS', 'CLM.STAT', 'CLM_YR', 'REIMB_YR')
lux <- select(lux_raw_data_input, -to_remove)
lux <- select(lux, -FSA_TYP, -FLEXBEN_TYPE) # Removes 2 columns
# Prepare Intervals
SUB.INT <- data.frame(lux$CLM_DT - lux$RCPT_DT)
colnames(SUB.INT) <- "Submission interval"
RMB.INT <- data.frame(lux$REIMB_DT - lux$CLM_DT)
colnames(RMB.INT) <- "Reimbursement interval"
# Remove date columns from tptclm.raw and replace with intervals
date.col.index <- c(1,3,6,7)
lux.cbind <- cbind.data.frame(lux[, -date.col.index], SUB.INT, RMB.INT)
# Prepare Integer Encoding Against the Types
CLM.TYP <- data.frame(unique(lux.cbind$CLAIM_TYPES),
c(1:length(unique(lux.cbind$CLAIM_TYPES))))
WK.DY <- data.frame(unique(lux.cbind$RCPT_DAY),
c(1:length(unique(lux.cbind$RCPT_DAY))))
DY.TAG <- data.frame(unique(lux.cbind$DAY_TAG),
c(1:length(unique(lux.cbind$DAY_TAG))))
# using %l% as a lookup function for matching
CLM.TYP.N <- data.frame(lux.cbind$CLAIM_TYPES %l% CLM.TYP)
colnames(CLM.TYP.N) <- "CLM.TYP.N"
WK.DY.N <- data.frame(lux.cbind$RCPT_DAY %l% WK.DY)
colnames(WK.DY.N) <- "WK.DY.N"
DY.TAG.N <- data.frame(lux.cbind$DAY_TAG %l% DY.TAG)
colnames(DY.TAG.N) <- "DY.TAG.N"
# Remove categorical columns from tptclm.sel and replace with integer encoded columns
cat.col.index <- c(2:4)
lux.num <- cbind.data.frame(lux.cbind[, -cat.col.index],
CLM.TYP.N,
WK.DY.N,
DY.TAG.N)
return(lux.num)
}
# SPLITS INTO DIFFERENT KINDS OF CLUSTERS
lux.num.ALL <- data_preprocessing(lux_raw)
lux.num.FLXI <-  data_preprocessing(lux_raw_flxi)
lux.num.BIRT <-  data_preprocessing(lux_raw_birt)
lux.num.BIRT <- select(lux.num.BIRT, -CLM.TYP.N) # Removes CLMTYP Columns because unnecessary
lux.num.MIIS <-  data_preprocessing(lux_raw_miis)
lux.num.MIIS <- select(lux.num.MIIS, -CLM.TYP.N) # Removes CLMTYP Columns because unnecessary
get_between_cluster_dist <- function(centroids) {
centroid_dist <- dist(centroids,  method = "euclidean", upper=T)
csum <- colSums(as.matrix(centroid_dist))
betweenDist <- (as.matrix(unname(csum)) / (nrow(centroids) - 1))
return(betweenDist)
}
do_elbow_plot <- function(lux.input.data) {
# Normalisation of Data
lux.norm<- sapply(lux.input.data,scale)
set.seed(1)
k.max <- 15
wss <- (nrow(lux.norm)-1)*sum(apply(lux.norm,2,var))
for (i in 2:k.max) wss[i] <- sum(kmeans(lux.norm,
centers=i,
iter.max = 15, algorithm = "Hartigan-Wong")$withinss)
plot(1:k.max, wss, type="b", xlab="Number of Clusters K",
ylab="Total within-cluster sum of squares",
main="Elbow Plot to find Optimal Number of Clusters",
pch=19, cex=1)
}
get_centroids <- function(lux.input.data, clusters) {
# Normalisation of Data
lux.norm<- sapply(lux.input.data,scale)
luxkmeans <- kmeans(lux.norm,clusters)
#number of records in each cluster
print("Cluster Sizes:")
print(luxkmeans$size) #smallest cluster often is the "outlying cluster"
# Cluster Centroids
centroids <- luxkmeans$centers
return(centroids)
}
get_cluster_numbers <- function(lux.input.data, clusters, luxkmeans) {
lux.clustn<-data.frame(luxkmeans$cluster) #dataframe of cluster membership for each record
# Rename to ClusterNumber
colnames(lux.clustn) <- "ClusterNumber"
# Add ClusterNumber to dataset
lux.clustn<-cbind.data.frame(lux.input.data,
lux.clustn)
return(lux.clustn)
}
do_k_means_proper <- function(lux.input.data, clusters) {
# Normalisation of Data
set.seed(1)
lux.norm<- sapply(lux.input.data, scale)
luxkmeans <- kmeans(lux.norm, clusters)
return(luxkmeans)
}
do_k_means <- function(lux.input.data, clusters, luxkmeans) {
#number of records in each cluster
lux.norm<- sapply(lux.input.data, scale)
#dist between cluster centers
print("Distance Between Cluster Centers")
print(dist(luxkmeans$centers))
lux.clustn<-data.frame(luxkmeans$cluster) #dataframe of cluster membership for each record
# Rename to ClusterNumber
colnames(lux.clustn) <- "ClusterNumber"
# Start Plotting Cluster Plot (Uncomment)
clusplot(lux.norm, luxkmeans$cluster,
color=TRUE, shade=TRUE,
labels=2, lines=1)
lux.centers <- luxkmeans$centers[luxkmeans$cluster,]
lux.dist <- sqrt(rowSums((lux.norm - lux.centers)^2)) #euclidean distance
# calculate mean distances by cluster:
# apply mean to distance for each cluster
mdist.km <- tapply(lux.dist, luxkmeans$cluster, mean)
# divide each distance by the mean for its cluster:
distscore.km <- lux.dist/(mdist.km[luxkmeans$cluster])
distfact.km <- data.frame(distscore.km)
colnames(distfact.km) <- "DIST.FACTOR"
# Distance Score Percentile
scoreperc.km <- data.frame((distscore.km-min(distscore.km))/(max(distscore.km) - min(distscore.km)))
colnames(scoreperc.km) <- "SCORE.PERC"
lux.clustdn<- cbind.data.frame(lux.clustn, distfact.km, scoreperc.km)
# Predetermine Threshold Percentile
P <-0.8
#Order datapoints by distance from cluster centers
lux.distorder <- order(distscore.km, decreasing = T)
lux.clustdn.order <- lux.clustdn[lux.distorder,]
lux.clustdn.order$rank <- rank(-lux.clustdn.order$SCORE.PERC)
# Plot Outliers
plot(x=scoreperc.km,
y=distscore.km,
main="Outliers based on %",
xlab="Distance Score Percentile",
ylab="Distance Score",
col=ifelse(scoreperc.km > P, "red", "blue"), #color
pch=13) # point corrector
return(scoreperc.km)
}
get_risk_clustered_results <- function(scores) {
lux.risk <- ifelse(scores > 0.8,1,0)
colnames(lux.risk) <- "RISK_CLUSTERED"
return(lux.risk)
}
plot_radar_plot <- function(centroids, title) {
x <- melt(centroids)
colnames(x) <- c("Cluster","Params","Distance")
distance <- reorder(x$Params, x$Cluster)
cluster <-as.factor(x$Cluster)
radar <- ggplot(x, aes(y = Distance, x = distance,
group = cluster, colour = cluster)) +
coord_polar() + geom_point() + geom_path() +
scale_colour_manual(values = c("#EA6A47", "#AC3E31", "#0091D5", "#DBAE58", "#6AB187", "#20283E", "#488A99", "#B3C100") )  +
theme_bw() + ggtitle(paste(c("Between Cluster Centroids -", title), collapse=" "))
return(radar)
}
plot_between_cluster_distance <- function(centroids, title) {
x <- get_between_cluster_dist(centroids)
y <- as.data.frame(x)
colnames(y) <- "Distance"
Cluster <- rownames(y)
line <- ggplot(y, aes(x=Cluster, y=Distance, group=1)) +
geom_line() + geom_path() + geom_point() +
ggtitle(paste(c("Between Cluster Distance -", title), collapse=" ")) +
ylim(1, 5)
return(line)
}
do_elbow_plot(lux.num.ALL)
# Assign the "optimal" number to a variable
C<- 8
abline(v = C, lty =2)
all.kmeans <- do_k_means_proper(lux.num.ALL, C)
all.scoreperc <- do_k_means(lux.num.ALL, C, all.kmeans)
all.centroids <- all.kmeans$centers
get_between_cluster_dist(all.centroids)
all.risk.results <- get_risk_clustered_results(all.scoreperc)
all.risk.cbind <- cbind.data.frame(lux_raw, all.risk.results)
all.outliers <- filter(all.risk.cbind, RISK_CLUSTERED == 1)
nrow(all.outliers)  # Number of outliers
write.csv(all.outliers,"data/RiskClusteredData - All.csv")
do_elbow_plot(lux.num.FLXI)
# Assign the "optimal" number to a variable
C<- 7
abline(v = C, lty =2)
flxi.kmeans <- do_k_means_proper(lux.num.FLXI, C)
flxi.scoreperc <- do_k_means(lux.num.FLXI, C, flxi.kmeans)
flxi.centroids <- flxi.kmeans$centers
get_between_cluster_dist(flxi.centroids)
flxi.risk.results <- get_risk_clustered_results(flxi.scoreperc)
flxi.risk.cbind <- cbind.data.frame(lux_raw_flxi, flxi.risk.results)
flxi.outliers <- filter(flxi.risk.cbind, RISK_CLUSTERED == 1)
nrow(flxi.outliers)
write.csv(flxi.outliers,"data/RiskClusteredData - FLXI.csv")
do_elbow_plot(lux.num.BIRT)
# Assign the "optimal" number to a variable
C<- 7
abline(v = C, lty =2)
birt.kmeans <- do_k_means_proper(lux.num.BIRT, C)
birt.scoreperc <- do_k_means(lux.num.BIRT, C, birt.kmeans)
birt.centroids <- birt.kmeans$centers
get_between_cluster_dist(birt.centroids)
birt.risk.results <- get_risk_clustered_results(birt.scoreperc)
birt.risk.cbind <- cbind.data.frame(lux_raw_birt, birt.risk.results)
birt.outliers <- filter(birt.risk.cbind, RISK_CLUSTERED == 1)
write.csv(birt.outliers,"data/RiskClusteredData - BIRT.csv")
do_elbow_plot(lux.num.MIIS)
# Assign the "optimal" number to a variable
C<- 7
abline(v = C, lty =2)
miis.kmeans <- do_k_means_proper(lux.num.MIIS, C)
miis.scoreperc <- do_k_means(lux.num.MIIS, C, miis.kmeans)
miis.centroids <- miis.kmeans$centers
miis.cluster.numbers <- get_cluster_numbers(lux.num.MIIS, C, miis.kmeans)
get_between_cluster_dist(miis.centroids)
miis.risk.results <- get_risk_clustered_results(miis.scoreperc)
miis.risk.cbind <- cbind.data.frame(lux_raw_miis, miis.risk.results)
miis.outliers <- filter(miis.risk.cbind, RISK_CLUSTERED == 1)
nrow(miis.outliers)
write.csv(miis.outliers,"data/RiskClusteredData - MIIS.csv")
lux_all_labelled <- merge(all.outliers, lux_validated_raw, by="CLM_REF")
#Merge the 2 claim columns
lux_all_labelled$FLEXBEN_TYPE.x <- lux_all_labelled$FLEXBEN_TYPE.x %>% replace_na("")
lux_all_labelled$CLAIM_TYPES <- paste(lux_all_labelled$FSA_TYP.x, lux_all_labelled$FLEXBEN_TYPE.x)
lux_all_labelled <- select(lux_all_labelled, -FSA_TYP.x, -FLEXBEN_TYPE.x)
nrow(all.outliers)
we_0_they_0 <- lux_all_labelled %>% filter(RISK_CLUSTERED==0, RISK==0)
we_0_they_1 <- lux_all_labelled %>% filter(RISK_CLUSTERED==0, RISK==1)
we_1_they_0 <- lux_all_labelled %>% filter(RISK_CLUSTERED==1, RISK==0)
we_1_they_1 <- lux_all_labelled %>% filter(RISK_CLUSTERED==1, RISK==1)
mismatch_flex_type <- we_1_they_0 %>% group_by(CLAIM_TYPES) %>% summarise(n())
lux.match <- ifelse(lux_all_labelled$RISK == lux_all_labelled$RISK_CLUSTERED, 1, 0)
match.perc <- sum(lux.match)/sum(all.outliers$RISK_CLUSTERED) # <- Matching percentage
print(match.perc)
outlier_flexben_type <- we_1_they_0 %>% group_by(CLAIM_TYPES) %>% count(CLAIM_TYPES)
# Outliers between Flexible Benefits
plot.flxi.ol <- ggplot(outlier_flexben_type, aes(x=CLAIM_TYPES, y=n))
plot.flxi.ol + geom_bar(stat="identity", fill="#DBAE58", colour="black")+
labs(title="Outliers from Flexible Benefits",
x = "Claim Type", y = "Number of Claims") +
theme(plot.title = element_text(size = 14, face = "bold"),
axis.text.x=element_text(colour="black", size = 9),
axis.text.y=element_text(colour="black", size = 9),
legend.position = "bottom",
legend.text = element_text(colour="#000000", size=7, face="bold"))
plot_between_cluster_distance(all.centroids, "ALL")
plot_radar_plot(all.centroids, "ALL")
validated.risk <- filter(lux_validated_raw, lux_validated_raw$RISK == 1)
clustering.risk.mean.clm.amt <- mean(all.outliers$CLM_AMT)
validated.risk.mean.clm.amt <- mean(validated.risk$CLM_AMT)
validated.non.risk <- filter(lux_validated_raw, lux_validated_raw$RISK == 0)
cluster.non.risk <- filter(all.risk.cbind, RISK_CLUSTERED == 0)
clustering.nonrisk.mean.clm.amt <- mean(cluster.non.risk$CLM_AMT)
validated.nonrisk.mean.clm.amt <- mean(validated.non.risk$CLM_AMT)
all.mean.claim.amts <- c(clustering.risk.mean.clm.amt, validated.risk.mean.clm.amt, clustering.nonrisk.mean.clm.amt, validated.nonrisk.mean.clm.amt)
all.data.set <- c("Clustering Results", "Validated Dataset","Clustering Results", "Validated Dataset")
all.risk <- c("Risk", "Risk", "Non Risk", "Non Risk")
all.group.dataframe <- data.frame(all.mean.claim.amts, all.data.set, all.risk)
colnames(all.group.dataframe) <- c("Mean Claim Amount", "Dataset", "Type of Risk")
p <- ggplot(data=all.group.dataframe, aes(x=`Type of Risk`, y=`Mean Claim Amount`, fill=`Dataset`)) +
geom_bar(stat="identity", position =position_dodge())+
scale_fill_manual(values = c(`Clustering Results` = "#DBAE58",  `Validated Dataset`= "#488A99"))+
guides(fill=guide_legend(title="Legend"))
p
total <- merge(flxi.outliers,lux_validated_raw,by="CLM_REF")
lux.match <- ifelse(total$RISK == total$RISK_CLUSTERED, 1, 0)
match.perc <- sum(lux.match)/sum(flxi.outliers$RISK_CLUSTERED) # <- Matching percentage
match.perc
plot_between_cluster_distance(flxi.centroids, "FLXI")
plot_radar_plot(flxi.centroids, "FLXI")
total <- merge(birt.outliers,lux_validated_raw,by="CLM_REF")
lux.match <- ifelse(total$RISK == total$RISK_CLUSTERED, 1, 0)
match.perc <- sum(lux.match)/sum(birt.outliers$RISK_CLUSTERED) # <- Matching percentage
match.perc
get_between_cluster_dist(birt.centroids)
plot_between_cluster_distance(birt.centroids, "BIRT")
plot_radar_plot(birt.centroids, "BIRT")
total <- merge(miis.outliers,lux_validated_raw,by="CLM_REF")
lux.match <- ifelse(total$RISK == total$RISK_CLUSTERED, 1, 0)
match.perc <- sum(lux.match)/sum(miis.outliers$RISK_CLUSTERED) # <- Matching percentage
match.perc
get_between_cluster_dist(miis.centroids)
plot_between_cluster_distance(miis.centroids, "MIIS")
plot_radar_plot(miis.centroids, "MIIS")
miis.outlier.cluster <- filter(miis.cluster.numbers, miis.cluster.numbers$ClusterNumber ==1)
summary(miis.outlier.cluster$CLM_AMT)
miis.cluster.numbers
miis.groupby.cluster <- miis.cluster.numbers %>% group_by(ClusterNumber) %>% summarise_all(funs(mean))
plot.flxi.ol <- ggplot(miis.groupby.cluster, aes(x=ClusterNumber, y=CLM_AMT))
plot.flxi.ol + geom_bar(stat="identity", fill="#DBAE58", colour="black")+
labs(title="Mean Claim Amounts between Clusters",
x = "Cluster", y = "Claim Amounts") +
scale_x_continuous(breaks=c(1:7)) +
theme(plot.title = element_text(size = 14, face = "bold"),
axis.text.x=element_text(colour="black", size = 9),
axis.text.y=element_text(colour="black", size = 9),
legend.position = "bottom",
legend.text = element_text(colour="#000000", size=7, face="bold"))
#Day distribution of all claims
dt.allclm = lux_raw %>% group_by(DAY_TAG) %>% count(DAY_TAG)
dt.allclm.pct = dt.allclm$n/ sum(dt.allclm$n)
dt.allclm = cbind.data.frame(dt.allclm, dt.allclm.pct)
#summarise % of DAY_TAG
dt.allclm %>% group_by(DAY_TAG) %>% summarise_each(funs(sum))
plot.dt.allclm <- ggplot(dt.allclm, aes(x=DAY_TAG, y=dt.allclm.pct))
plot.dt.allclm + geom_bar(stat="identity", fill="#DBAE58")+
labs(title="Distribution of ALL CLAIMS by Day",
y = "Percentage of Claims", x = "Day") +
theme(plot.title = element_text(size = 14, face = "bold"),
axis.text.x=element_text(colour="black", size = 9),
axis.text.y=element_text(colour="black", size = 9),
legend.position = "bottom",
legend.text = element_text(colour="#000000", size=7, face="bold"))
#Distribution of outliers
dt.birt = birt.outliers %>% group_by(DAY_TAG) %>% count(DAY_TAG)
dt.miis = miis.outliers %>% group_by(DAY_TAG) %>% count(DAY_TAG)
dt.flxi = flxi.outliers %>% group_by(DAY_TAG) %>% count(DAY_TAG)
ol.clm = do.call("rbind", list(dt.birt, dt.miis, dt.flxi))
ol.clm.pct = ol.clm$n/ sum(ol.clm$n)
dt.ol.clm = cbind.data.frame(ol.clm, ol.clm.pct)
#summarise % of DAY_TAG
dt.ol.clm %>% group_by(DAY_TAG) %>% summarise_each(funs(sum))
plot.dt.ol.clm <- ggplot(dt.ol.clm, aes(x=DAY_TAG, y=ol.clm.pct))
plot.dt.ol.clm + geom_bar(stat="identity", fill="#AC3E31")+
labs(title="Distribution of OUTLIERS by Day",
y = "Percentage of Claims", x = "Day") +
theme(plot.title = element_text(size = 14, face = "bold"),
axis.text.x=element_text(colour="black", size = 9),
axis.text.y=element_text(colour="black", size = 9),
legend.position = "bottom",
legend.text = element_text(colour="#000000", size=7, face="bold"))
packages = c('readr', 'dplyr', 'caret', 'qdapTools', 'mltools', 'data.table', 'DMwR', 'ROSE',
'rpart', 'C50', 'randomForest', 'rpart.plot', 'pROC')
for (p in packages){
if(!require(p, character.only = T)){
install.packages(p)
}
library(p,character.only = T)
}
# Set seed
set.seed(123)
validated <- read_csv('data/luxehorecaFSA(validated).csv',
col_types = cols(CLM_DT    = col_date(format = "%d/%m/%y"),
RCPT_DT   = col_date(format = "%d/%m/%y"),
REIMB_DT  = col_date(format = "%d/%m/%y"),
EA_DT     = col_date(format = "%d/%m/%y")))
outliers <- read_csv('data/RiskClusteredData - All.csv')
#Drop MIIS 3
missing <- validated %>% filter(FSA_TYP=='MIIS', FLEXBEN_TYPE==3)
validated <- validated[validated$CLM_REF!=missing$CLM_REF,]
to_remove <- c('EMP_ID', 'CLM_REF', 'CLM_SYS', 'CLM.STAT', 'CLM_YR', 'REIMB_YR')
lux <- dplyr::select(validated, -to_remove)
#Merge the 2 claim columns
lux$FLEXBEN_TYPE <- lux$FLEXBEN_TYPE %>% replace_na("")
lux$CLAIM_TYPES <- paste(lux$FSA_TYP, lux$FLEXBEN_TYPE)
lux <- dplyr::select(lux, -FSA_TYP, -FLEXBEN_TYPE)
RCPT.INT <- data.frame(lux$CLM_DT - lux$RCPT_DT)
colnames(RCPT.INT) <- "Submission_Interval"
RCPT.INT$`Submission_Interval` <- as.numeric(RCPT.INT$Submission_Interval)
REIMB.INT <- data.frame(lux$REIMB_DT - lux$CLM_DT)
colnames(REIMB.INT) <- "Reimbursement_Interval"
REIMB.INT$`Reimbursement_Interval` <- as.numeric(REIMB.INT$`Reimbursement_Interval`)
# Remove date columns from tptclm.raw and replace with intervals
date.col.index <- c(1,3,6,7)
lux.cbind <- cbind.data.frame(lux[, -date.col.index], RCPT.INT, REIMB.INT)
CLM.TYP <- data.frame(unique(lux.cbind$CLAIM_TYPES),
c(1:length(unique(lux.cbind$CLAIM_TYPES))))
WK.DY <- data.frame(unique(lux.cbind$RCPT_DAY),
c(1:length(unique(lux.cbind$RCPT_DAY))))
DY.TAG <- data.frame(unique(lux.cbind$DAY_TAG),
c(1:length(unique(lux.cbind$DAY_TAG))))
# using %l% as a lookup function for matching
CLM.TYP.N <- data.frame(lux.cbind$CLAIM_TYPES %l% CLM.TYP)
colnames(CLM.TYP.N) <- "CLM.TYP.N"
WK.DY.N <- data.frame(lux.cbind$RCPT_DAY %l% WK.DY)
colnames(WK.DY.N) <- "WK.DY.N"
DY.TAG.N <- data.frame(lux.cbind$DAY_TAG %l% DY.TAG)
colnames(DY.TAG.N) <- "DY.TAG.N"
# Remove categorical columns from tptclm.sel and replace with integer encoded columns
cat.col.index <- c(2:3, 5)
lux.num <- cbind.data.frame(lux.cbind[, -cat.col.index],
CLM.TYP.N,
WK.DY.N,
DY.TAG.N,
CLM_REF = validated$CLM_REF)
#Merge the outliers and the validated risk columns
merged_db <- left_join(lux.num, outliers, by="CLM_REF")
risk_clust <- merged_db$RISK_CLUSTERED
risk_clust[is.na(risk_clust)] <- 0
merged_db$RISK_CLUSTERED <- risk_clust
merged_db$Risk_Final <- merged_db$RISK + merged_db$RISK_CLUSTERED
merged_db$Risk_Final[merged_db$Risk_Final ==2] <- 1
final_df <- lux.num
final_df$risk_final <- merged_db$Risk_Final
final_df <- final_df %>% dplyr::select(-c(RISK, CLM_REF))
write_csv(final_df, 'data/luxehorecaFSA(Supervised).csv')
#lux_final   <- read_csv('data/luxehorecaFSA(Supervised).csv')
lux_final <- final_df
# Check distribution of data before balancing
lux_final %>% group_by(risk_final) %>% summarise(n())
# Balance Dataset by risk label by oversampling, using ROSE method
data_balanced_rose <- ROSE(risk_final ~ ., data = lux_final, seed=1, p=0.5 )$data
# Inspect balance of dataset
data_balanced_rose %>% group_by(risk_final) %>% summarise(n())
#Models used: Recursive Partitioning, C50, RandomForest, Logistic Regression
# Evaluate using k fold cross validation. For better evaluation of model on new data
num_folds = 10
folds <- createFolds(factor(data_balanced_rose$risk_final), k = num_folds, list = FALSE)
data_balanced_rose$FOLD <- folds
metrics <- data.frame(matrix(0, nrow=6, ncol=4)) # TO hold sum values (TP,FP,TN,FN) for all test folds
names(metrics)    <- c('TP', 'FP', 'TN', 'FN')
rownames(metrics) <- c('Rpart', 'C50', 'Random Forest', "Logit", "Logit Backward", "Logit Forward")
# Use each fold as test set, whilst using all others to as fit trees
# Sum TP,FP,TN,FN numbers into `metrics` df created above
cat("Training for ", num_folds, " folds **MAY TAKE A FEW SECONDS**\n")
for(i in 1:10) {
cat("\n\tStarting fold ", i)
trn <- select(filter(data_balanced_rose, FOLD == i), -c('FOLD'))
tst <- select(filter(data_balanced_rose, FOLD != i), -c('FOLD'))
#==================================== Cpart
lux_rpart <- rpart(risk_final ~ ., data=trn, method="class", minbucket=50, maxdepth=5)
tst_rpart_pred <- predict(lux_rpart, tst, type="class")
rpart_cm <- confusionMatrix(table(tst_rpart_pred, tst$risk_final), positive = "1", mode="prec_recall")
metrics[1,] <- metrics[1,] + rpart_cm$table
#==================================== C5.0
lux_c50 <- C5.0(as.factor(risk_final)~., data = trn, rules= TRUE)
tst_c50_pred <- predict(lux_c50, tst, type="class")
c50_cm <- confusionMatrix(table(tst_c50_pred, tst$risk_final), positive = "1", mode="prec_recall")
metrics[2,] <- metrics[2,] + c50_cm$table
#==================================== Random Forrest
lux_randf <-randomForest(as.factor(risk_final)~., data = trn, ntree=500, mtry=4, nodesize=5, importance=TRUE)
lux_randf_pred <- predict(lux_randf, tst, type="class", main="Variable Importance Plot")
ranf_cm <- confusionMatrix(table(lux_randf_pred, tst$risk_final), positive = "1", mode="prec_recall")
metrics[3,] <- metrics[3,] + ranf_cm$table
#==================================== Logistic Regression
lux_logit     <- glm(risk_final ~., family = "binomial", data = trn)
lux_logit_bwd <- step(lux_logit, direction = "backward", trace=F)
lux_logit_fwd <- step(lux_logit, direction = "forward", trace=F)
lux_logit_pred     <- ifelse(predict(lux_logit, tst, type = "response") > 0.5, 1, 0)
lux_logit_bwd_pred <- ifelse(predict(lux_logit_bwd, tst, type = "response") > 0.5, 1, 0)
lux_logit_fwd_pred <- ifelse(predict(lux_logit_fwd, tst, type = "response") > 0.5, 1, 0)
logit_cm     <- confusionMatrix(table(lux_logit_pred, tst$risk_final), positive = "1", mode="prec_recall")
logit_bwd_cm <- confusionMatrix(table(lux_logit_bwd_pred, tst$risk_final), positive = "1", mode="prec_recall")
logit_fwd_cm <- confusionMatrix(table(lux_logit_fwd_pred, tst$risk_final), positive = "1", mode="prec_recall")
metrics[4,] <- metrics[4,] + logit_cm$table
metrics[5,] <- metrics[5,] + logit_bwd_cm$table
metrics[6,] <- metrics[6,] + logit_fwd_cm$table
}
#### Confusion Matrix: Sum of TP, FP, TN, FN of each folds's test set performance
#### # Evaluate by balanced measure, Matthews correlation coefficient
for(i in 1:nrow(metrics)){
TP <- metrics$TP[i]
FP <- metrics$FP[i]
TN <- metrics$TN[i]
FN <- metrics$FN[i]
precision <- TP/(TP+FP)
recall    <- TP/(TP+FN)
f1        <- 2 * (precision*recall)/(precision+recall)
mcc       <- mcc(TP=TP, FP=FP, TN=TN, FN=FN)
cat(rownames(metrics)[i], "\n")
cat("\t   TP: ", TP, "\tFP: ", FP, "\n")
cat("\t   FN: ", FN, "\tTN: ", TN, "\n")
cat("\tPrecision:", precision, "\n\tRecall:   ", recall, "\n\tF1 Score: ", f1, "\n\n")
}
# Rule extrction for rpart
rpart.plot::rpart.plot(lux_rpart, type=3, extra=101)
# Rule extraction for C50
summary(lux_c50)
varImpPlot(lux_randf, type=2)
# Closer look at Logistic Regression models
options(scipen = 999)
print("==================================== LUX LOGIT =========================================")
summary(lux_logit)
print("==================================== LOGIT backward ====================================")
summary(lux_logit_bwd)
print("==================================== LOGIT forward =====================================")
summary(lux_logit_fwd)
